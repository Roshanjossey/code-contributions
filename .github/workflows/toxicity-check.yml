name: Check for Toxic Content

on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  moderate-content:
    runs-on: ubuntu-latest  

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3  

      - name: Set up Python
        uses: actions/setup-python@v3
        with:
          python-version: "3.x"  

      - name: Cache Hugging Face models
        uses: actions/cache@v3
        with:
          path: ~/.cache/huggingface  
          key: huggingface-toxicity-model

      - name: Install dependencies
        run: pip install transformers torch  

      - name: Get changed files
        id: changed-files
        uses: tj-actions/changed-files@v35  # Detecta archivos modificados en la PR
        with:
          files: "**/*.html"

      - name: Run toxicity check
        run: python scripts/check_toxicity.py ${{ steps.changed-files.outputs.all_modified_files }}

      - name: Auto-approve PR if no offensive content is found
        if: success()
        uses: hmarr/auto-approve-action@v3
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Comment on PR if offensive content is detected
        if: always() && steps.changed-files.outputs.all_modified_files != ''
        run: echo "ðŸš¨ Offensive content detected. Please review your PR." | tee comment.txt

      - name: Post comment on PR
        if: always() && steps.changed-files.outputs.all_modified_files != ''
        uses: thollander/actions-comment-pull-request@v2
        with:
          message: file://comment.txt
